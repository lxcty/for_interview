{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "此篇为spark-sql的处理方式，故先读取两个样本数据用来做例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|user_id|movie_id|rating|\n",
      "+-------+--------+------+\n",
      "|      0|       2|     3|\n",
      "|      0|       3|     1|\n",
      "|      0|       5|     2|\n",
      "|      0|       9|     4|\n",
      "|      0|      11|     1|\n",
      "|      0|      12|     2|\n",
      "|      0|      15|     1|\n",
      "|      0|      17|     1|\n",
      "|      0|      19|     1|\n",
      "|      0|      21|     1|\n",
      "|      0|      23|     1|\n",
      "|      0|      26|     3|\n",
      "|      0|      27|     1|\n",
      "|      0|      28|     1|\n",
      "|      0|      29|     1|\n",
      "|      0|      30|     1|\n",
      "|      0|      31|     1|\n",
      "|      0|      34|     1|\n",
      "|      0|      37|     1|\n",
      "|      0|      41|     2|\n",
      "+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df = spark.read.text(\"/usr/local/spark/spark-2.4.0-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\") \\\n",
    "    .withColumn(\"info\",F.expr(\"split(value,'::')\"))\\\n",
    "    .selectExpr(\"cast(info[0] as int) as user_id\",\"cast(info[1] as int) as movie_id\",\"cast(info[2] as int) as rating\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------------+\n",
      "|count(DISTINCT user_id)|count(DISTINCT movie_id)|\n",
      "+-----------------------+------------------------+\n",
      "|                     30|                     100|\n",
      "+-----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.count()\n",
    "df.selectExpr(\"count(distinct user_id)\", \"count(distinct movie_id)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#生成两个表，并进行表注册\n",
    "df.registerTempTable(\"tdl_table_total\")\n",
    "df.filter(\"movie_id<70\").registerTempTable(\"tdl_table_1\")\n",
    "df.filter(\"movie_id>30\").registerTempTable(\"tdl_table_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、如何进行两个表的拼接（包括按行和按列）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#按行，需保证两个表格字段相同，union选取不重复的值进行操作，union all则保留重复值\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select * from tdl_table_1\n",
    "    union\n",
    "    select * from tdl_table_2\n",
    "    \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2087"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select * from tdl_table_1\n",
    "    union all\n",
    "    select * from tdl_table_2\n",
    "    \"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2、从表1中排除表2中存在的movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用left join\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select a.* from \n",
    "      tdl_table_1 a\n",
    "    left join\n",
    "      tdl_table_2 b\n",
    "    on a.movie_id=b.movie_id\n",
    "    where b.movie_id is null\n",
    "    \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用not exists\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select a.* from \n",
    "      tdl_table_1 a\n",
    "    where not exists (select movie_id from tdl_table_2 b where a.movie_id=b.movie_id)\n",
    "    \"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、选择每个user_id评分最高的movie_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|user_id|favorite_movie|\n",
      "+-------+--------------+\n",
      "|     28|            12|\n",
      "|     26|             7|\n",
      "|     27|            18|\n",
      "|     12|            17|\n",
      "|     22|            22|\n",
      "|      1|            62|\n",
      "|     13|            18|\n",
      "|      6|            25|\n",
      "|     16|            51|\n",
      "|      3|            51|\n",
      "|     20|            22|\n",
      "|      5|            55|\n",
      "|     19|            32|\n",
      "|     15|            46|\n",
      "|      9|             7|\n",
      "|     17|            17|\n",
      "|      4|            29|\n",
      "|      8|            29|\n",
      "|     23|            27|\n",
      "|      7|            25|\n",
      "+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 利用窗口函数\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select user_id,movie_id as favorite_movie from\n",
    "      (select *,row_number() over(partition by user_id order by rating desc) as rn from tdl_table_1)\n",
    "    where rn=1\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、去除重复项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用distinct，但是这种方法可能会导致数据倾斜，效率不高\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select distinct * from\n",
    "    (select * from tdl_table_1\n",
    "    union all\n",
    "    select * from tdl_table_2)t\n",
    "    \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用窗口函数，需要根据具体场景来定，排序效率不高\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select user_id,movie_id,rating from\n",
    "      (select *,row_number() over(partition by user_id,movie_id,rating order by rating) as rn from\n",
    "        (select * from tdl_table_1\n",
    "        union all\n",
    "        select * from tdl_table_2)t\n",
    "      )t1\n",
    "    where rn=1\n",
    "    \"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1501"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用group by，改善数据倾斜\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select user_id,movie_id,rating from\n",
    "      (select user_id,movie_id,rating,count(*) as cnt from\n",
    "        (select * from tdl_table_1\n",
    "        union all\n",
    "        select * from tdl_table_2)t\n",
    "       group by user_id,movie_id,rating\n",
    "      ) \n",
    "    \"\"\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、求取两个movie_id被相同的人看过的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| pair|cnt|\n",
      "+-----+---+\n",
      "|29,51| 17|\n",
      "| 2,50| 15|\n",
      "| 6,50| 15|\n",
      "| 6,88| 15|\n",
      "|22,51| 14|\n",
      "| 6,94| 14|\n",
      "|50,88| 14|\n",
      "| 6,22| 14|\n",
      "|50,85| 14|\n",
      "|29,55| 14|\n",
      "|15,29| 14|\n",
      "| 6,63| 14|\n",
      "| 6,82| 14|\n",
      "|51,79| 14|\n",
      "| 6,12| 14|\n",
      "|22,66| 14|\n",
      "| 2,14| 13|\n",
      "|15,67| 13|\n",
      "|29,85| 13|\n",
      "|36,51| 13|\n",
      "+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 这种方式在数据集很大时不可取，相当于笛卡尔积\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select if(a.movie_id<b.movie_id, concat(a.movie_id,',',b.movie_id) ,concat(b.movie_id,',',a.movie_id)) as pair,\n",
    "    count(distinct a.user_id) as cnt from \n",
    "    \n",
    "      tdl_table_total a\n",
    "    join\n",
    "      tdl_table_total b\n",
    "      \n",
    "    on a.movie_id != b.movie_id\n",
    "    where a.user_id = b.user_id\n",
    "    group by pair\n",
    "    order by cnt desc\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| pair|cnt|\n",
      "+-----+---+\n",
      "|29,51| 17|\n",
      "| 2,50| 15|\n",
      "| 6,50| 15|\n",
      "| 6,88| 15|\n",
      "|50,85| 14|\n",
      "|51,79| 14|\n",
      "|15,29| 14|\n",
      "| 6,94| 14|\n",
      "|22,51| 14|\n",
      "|50,88| 14|\n",
      "| 6,63| 14|\n",
      "|29,55| 14|\n",
      "| 6,82| 14|\n",
      "| 6,12| 14|\n",
      "| 6,22| 14|\n",
      "|22,66| 14|\n",
      "|50,94| 13|\n",
      "|12,50| 13|\n",
      "|36,51| 13|\n",
      "|15,67| 13|\n",
      "+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 考虑到每个user_id关联的movie_id数量会较少（不可能看过所有的电影）\n",
    "# 以每个user_id来聚类，然后生成的movie_list中的电影两两关联度均为1\n",
    "# 实际场景中，需根据数据情况先过滤掉user_id只关联一个movie_id的情况或做些其他分组以减轻计算压力\n",
    "from itertools import combinations\n",
    "from pyspark.sql.types import * \n",
    "def array_cross(x):\n",
    "    r = list(combinations(x, 2))\n",
    "    return [\"%d,%d\"%(i[0],i[1]) if i[0]<i[1] else \"%d,%d\"%(i[1],i[0]) for i in r]\n",
    "   \n",
    "spark.udf.register(\"spark_func_array_cross\", array_cross, ArrayType(StringType()))\n",
    "#spark.sql(\"select user_id,spark_func_array_cross(collect_set(movie_id)) as movie_pair from tdl_table_total group by user_id\").printSchema()\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    select pair,count(pair) as cnt from\n",
    "      (select user_id,spark_func_array_cross(collect_set(movie_id)) as movie_pair from tdl_table_total group by user_id)t\n",
    "    lateral view explode(movie_pair) tf as pair\n",
    "    group by pair order by cnt desc\n",
    "    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}